\chapter{Perceptions in real spaces}

% **************************** Define Graphics Path **************************

\graphicspath{{Chapter5/plots/} {Chapter5/plots/examples/}}
    
There has been an explosive growth of deep learning technologies and their competency in the recent years, resulting in cross disciplinary use cases of deep learning enabled tools. In the area of computer vision and urban informatics, deep learning techniques have recently been used to predict whether urban scenes are likely to be considered beautiful, and it turns out that these techniques do so quite accurately. However, the technology falls short when it comes to generating actionable insights for AI assisted urban design. To support urban interventions, one needs to go beyond \emph{predicting} beauty, and tackle the challenge of \emph{recreating} beauty and \emph{explaining} the predictors of beauty.  Unfortunately, deep learning techniques have not been designed with that challenge in mind. Given their ``black-box nature'', they cannot even explain why a scene has been predicted to be beautiful. To partly fix that, we propose a deep learning framework (which we name  FaceLift) that is able to both \emph{beautify} existing Google Street views and \emph{explain} which urban elements make those transformed scenes beautiful. To quantitatively evaluate our framework, we cannot resort to any existing metric (as the research problem at hand has never been faced before) and need to  formulate new ones. These new metrics should ideally capture the presence (or absence) of elements that make urban spaces great. Upon a review of the urban planning literature, we identify four main metrics: walkability, green, openness, and visual complexity.  For all the four metrics, the beautified scenes meet the expectations set by the literature on what great spaces tend to be made of. The transformations and their explanations are also found to be very helpful in understanding interventions for beautification, which we validate using a 20-participant expert survey. These results suggest that, in the future, as our framework's components are further researched and become better and more sophisticated, it is not hard to imagine technologies that will be able to accurately and efficiently support architects and planners in the design of the spaces we intuitively love.


\section{Introduction}


Whether a street is considered beautiful is subjective, yet research has shown that there are specific urban elements that are universally considered beautiful: from greenery, to small streets, to memorable spaces~\cite{alexander1977pattern, quercia2014aesthetic,salesses2013collaborative}. These elements are those that contribute to the creation of what the urban sociologist Jane Jacobs called `urban vitality'~\cite{jacobs1961death}. 


Given that, it comes as no surprise that computer vision techniques can automatically analyse pictures of urban scenes and accurately determine the extent to which these scenes are, \emph{on average}, considered beautiful.  Deep learning has greatly contributed to increase these techniques' accuracy~\cite{dubey2016deep}.

However, urban planners and architects are interested in urban interventions and, as such, they wish to go beyond technologies that are only able to predict beauty scores. These interests stem from the fact that the spaces we live in can be linked with several aspects of human life such as mental health\cite{seresinhe2015quantifying}, inequality \cite{salesses2013collaborative} or cultural shifts \cite{10.3389/fphy.2018.00027}. They often called for technologies that would make easier to recreate beauty in urban design~\cite{de2008architecture}. Deep learning, by itself, is not fit for purpose. It is not meant to recreate beautiful scenes, not least because it cannot provide any explanation on why a scene is beautiful. 


To partly fix that, we propose a deep learning framework (which we name  FaceLift) that is able to both \emph{generate} a beautiful scene (or, better, \emph{beautify} an existing scene) and \emph{explain} why that scene is beautiful. This opens up a possibility of using technology in urban planning efforts like decision making based of subjective opinions, participatory urban planning and promotion of restorative urban design such as green spaces and walkable areas.  Through this work, we make two main contributions:

\begin{itemize}
    \item We propose a deep learning framework that is able to learn whether Google Street views are beautiful or not, and that, based on that training, is able to both \emph{beautify} existing views and \emph{explain} which urban elements  make these views beautiful (Section~\ref{sec:framework}). 
    
    \item We quantitatively evaluate whether the framework is able to actually produce beautified scenes (Section~\ref{sec:evaluation}). We do so by proposing a family of four urban design metrics that we have formulated based on a thorough review of the literature in urban planning. For all these four metrics, the framework passes with flying colors: with minimal interventions, beautified scenes are twice as walkable as the original scenes, for example. Also, after building an interactive tool with ``FaceLifted'' scenes in Boston and presenting it to twenty experts in architecture,  we found that the majority of them agreed on three main areas of our work's impact: decision making, participatory urbanism, and promotion of restorative spaces among the general public. 
    
    %\item We find that a vast majority of participants see a general potential for the technology: 85\% of expert participants stated that it was probably better than existing tools used for \textit{``Participatory approaches on urban planning''}. 70\% expressed the  same opinions about its utilization for \textit{``decision making''}, also 70\% saw potential in its ability to \textit{``promote green cities''}.  
\end{itemize}
For sake of brevity, we will use the term 'Urban Scene` through out the paper to address an arbitrary Google Street View image. The image is fetched from a particular latitude and longitude point on the map. 
In the rest of the paper we explore related literature across various tracks of urban perceptions and urban beauty in Section \ref{sec:related}. We then describe in detail the Facelift framework in Section \ref{sec:framework}. The evaluation of the framework is described in detail in Section \ref{sec:evaluation} We conclude by pointing out some limitations and biases that might well guide future work (Section~\ref{sec:discussion}).






\section{Related Work}
\label{sec:related}
%\ns{This needs more work.}
Previous work has focused on collecting ground truth data about how people perceive urban spaces, on predicting urban qualities from visual data, and on generating synthetic images that enhance a given quality (e.g., beauty). 


\mbox{}\\
\noindent
\textbf{Ground truth of urban perceptions.} So far the most detailed studies of perceptions of urban environments and their visual appearance have relied on personal interviews and observation of city streets: for example, some researchers relied on annotations of video recordings by experts~\cite{sampson04seeing}, while others have used participant ratings of simulated (rather than existing) street scenes~\cite{lindal2012}. The web has recently been used to survey a large number of individuals. Place Pulse is a website that asks a series of binary perception questions (such as `Which place looks safer [between the two]?') across a large number of geo-tagged images~\cite{salesses2013collaborative}. In a similar way, Quercia \emph{et al.} collected pairwise judgments about the extent to which urban scenes are considered quiet, beautiful and happy~\cite{quercia2014aesthetic}. They were then able to analyze the scenes together with their ratings using image-processing tools, and found that the amount of greenery in any given scene was associated with all three attributes and that cars and fortress-like buildings were associated with sadness. Taken all together, their results pointed in the same direction: urban elements that hinder social interactions were undesirable, while elements that increase interactions were the ones that should be integrated by urban planners to retrofit cities for greater happiness. 

\mbox{}\\
\noindent
\textbf{Deep learning and the city.} Computer vision techniques have increasingly become more sophisticated. Deep learning techniques, in particular, have been recently used to accurately predict urban beauty~\cite{dubey2016deep,seresinhe2017using}, urban change~\cite{naik2017computer}, and even crime~\cite{DeNadai16}. These works also did some interesting analysis of the data to understand how safety, depression, beauty and other such dimensions are perceived across urban spaces.  \cite{dubey2016deep} also utilized deep learning methods to train models capable of comparing two urban images for their perception values in terms of beauty et.al. However even these works did not dive into the reasoning aspect of these models.

\mbox{}\\
\noindent
\textbf{Generative models.} Deep learning has recently been used not only to analyze existing images but also to generate new ones. n the past couple of years, there have been papers which exploit generative version of neural nets to delve into the aspects of explainability. Nguyen \emph{et al.}~\cite{nguyen2016synthesizing} used generative networks to create a natural-looking image that maximizes a specific neuron. In theory, the resulting image is the one that ``best activates'' the neuron under consideration (e.g., that associated with urban beauty). In practice, it is still a synthetic template that needs further processing to look realistic. \mbox{} \\

\mbox{}
To sum up, a lot of work has gone into collecting ground truth data about how people tend to perceive urban spaces, and into building accurate predictions models of urban qualities. However,  little work has gone into models that generate realistic urban scenes and that offer human-interpretable explanations of what they generate. 




\section{FaceLift Framework}
\label{sec:framework}

\begin{table}[t]
    \resizebox{0.7\linewidth}{!}{
        \begin{tabular}{l|p{8cm}}
            \textbf{Symbol} & \textbf{Meaning}\\
            $I_i$    & Original urban scene \\
            $Y$    & Set of annotation classes for urban scenes (e.g., beautiful, ugly)\\
            $y_i$    & Annotation class in $Y$ (e.g., beautiful) \\
            $\hat{I_j}$ & Template scene (synthetic image) \\
            $I'$ & Target Image \\
            $C$ & Beauty Classifier \\
            %$R$ & Images acquired by rotating Street view camera \\
            %$T$ & Images acquired by translating street view camera\\
            %$\rho$ & Similarity bound below which smart augmentation chooses translated images \\
            & \\
            %			\textbf{term} & \textbf{stands for}\\
            %			\textit{Template Image} $\hat{I_j}$    & A synthetic transformation of input image $I$ towards the class $y_j$ \\
            %			\textit{Target Image} $I'$    & The natural image which is most visually similar to the template image \\
            %		%	\textit{ Data Clustering}    & A process which groups images in $X$ according to visual similarity (e.g urban vs rural)\\
            %			\textit{Data Augmentation}    & A process of data expansion which looks for images taken in the surroundings of the georeferenced images in $X$\\
            %			\textit{Classifier}   & A deep-learning framework that is able to classify images into one of the classes in $Y$\\
            %			\textit{Generator} $(GAN)$    & A deep-learning based image generator \\% framework to produce images similar to the ones in  $X$\\
            %			$DGN-AM$    & A framework that, given the GAN and the Classifier, transforms an input image into the template image.\\
    \end{tabular}}
    \caption{Notations}\label{notations}
\end{table}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{facelift-pipeline-2x.png}
    \caption{A simplistic end to end illustration of the FaceLift framework.}
    \label{fig:framework}
\end{figure*}

%****************************************
The goal of FaceLift is to take as input a geo-located urban scene and give as output its transformed (beautified) version. 
To that end, it performs in five steps: 
\begin{itemize}
    \item \textbf{Curating urban scenes} It is common knowledge that deep learning systems need immense amount of data. In this first step we try to develop a sound framework for curating and augmenting annotated images, on which the model could be trained.
    \item \textbf{ Training a beauty classifier} To generate beauty, you first need a reliable model that could learn the representation of beauty. To achieve this, we train a deep learning model that could distinguish beautiful urban scenes from non-beautiful urban scenes. 
    \item \textbf{Generating a synthetic beautified scene} Based on the learned representation of beauty, we train a Generative model which could augment the beauty of an input urban scene. 
    \item \textbf{Retrieving a realistic beautified scene} as showcased in Figure \ref{fig:framework}, the generated images are representations of beautified input urban scene in a latent space. This latent representation needs to be transformed back to a realistic looking image, using retrieval.
    \item \textbf{Identifying the urban elements characterizing the beautified scene} In the final step, the framework explains changes introduced in the transformation process in terms of literature-driven urban design metrics, and quantifies these changes as metrics for urban beauty.
\end{itemize}
%1) curating urban scenes; 2) training a beauty classifier; 3) generating a synthetic beautified scene; 4) returning a realistic beautified scene; and 5) identifying the urban elements characterizing the beautified scene. 
%\ns{You need a bit more detail here. First sentence says what you are going to achieve. But you need to motivate why these specific five steps will achieve this goal.}


%****************************************
\subsection*{Step 1 Curating Urban Scenes}
To begin with, we need highly curated training data with labels reflecting urban beauty. We start with the  Place Pulse dataset that contains 100k Google Street Views across 56 cities around the world~\cite{dubey2016deep}. These scenes are labeled in terms of whether the corresponding places are likely to be perceived beautiful, depressing, rich, and safe. We focus only on those scenes that are labeled in terms of beauty and that have at least three judgments. This leave us with roughly  20,000 scenes. To transform judgments into beauty scores, we use the TrueSkill algorithm~\cite{herbrich2007trueskill}, which gives us a way of partitioning the scenes into two sets (Figure \ref{fig:Trueskill}): one containing beautiful scenes, and the other containing ugly scenes. The resulting set of scenes is too small for training any deep learning module without avoiding over-fitting though. As such, we need to augment such a set. 

We do so in two ways. First, we feed each scene's location into the Google Streetview API to obtain  the snapshots of the same location at different camera angles (i.e., at $\theta \in {-30^{\circ}, -15^{\circ} , 15^{\circ} , 30^{\circ} }$). However, the resulting dataset is still too small for robust training. Therefore, again, we feed each scene's location into the Google Streetview API, but now we do so to obtain other scenes at  distance $d \in \{10,20,40,60\}$ meters.  This will greatly expand our set of scenes, but it might do so at the price of introducing scenes whose beauty scores have little to do with the original scene's. To fix that, we take only the scenes that are \emph{similar} to the original one (we call this way of augmenting ``conservative translation''). To compute the similarity between a pair of scenes, we represent the two scenes with visual features derived from the FC7 layer of PlacesNet and compute the similarity between the two corresponding feature vectors~\cite{zhou2014learning}. For all scenes at increasing distance $d \in \{10,20,40,60\}$ meters,  we take only those whose similarity scores with the original scene is above a threshold. In a conservative fashion, we choose that threshold to be the median similarity between rotated and original scenes (those of the first augmentation step). 

To make sure this additional augmentation has not introduced any unwanted noise, we consider  two sets of scenes: one containing those that have been taken during this last step, i.e. the one with high similarity to the original scenes (\emph{taken-set}), and the other containing those that have been filtered away (\emph{filtered-set}). Each scene is then scored with PlacesNet~\cite{zhou2014learning} and is represented with the five most confident scene labels. We then aggregate labels at set level, by computing each label's frequency on the \emph{taken-set} %minus that on the 
and on the \emph{filtered-set}. Finally, we characterize each label's propensity to be correctly augmented as: 
$ \textrm{prone}(label)= fr(label,\textrm{\emph{taken-set}}) - fr(label,\textrm{\emph{filtered-set}}).$
This reflects the extent to which a scene with a given label is prone to be augmented or not. From Figure~\ref{fig:augmentationSimilarity}, we find that, as one would expect, scenes that contain highways, fields and bridges can be augmented at increasing distances while still showing resemblances to the original scene; by contrast, scenes that contain gardens, residential neighborhoods , plazas, and skyscrapers cannot be easily augmented, as they are often found in high density parts of the city, where there is tremendous diversity within short distances. 

%\ns{You probably want to reword this: It is amenable to augmentation, not prone to. Also why is this being done at a set level? If you always find bridges next to viaducts and viaducts next to bridges, and if your translation step moves from one to another, then the set-level score will not detect this. If instead you check whether the placesnet\_label of the translated image is the same as that of the original, then you are OK, as long as placesnet\_label can be be believed.}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\columnwidth]{Trueskill.png}
    \caption{Frequency distribution of beauty scores. The red and green lines represent the thresholds below and above which images are considered ugly and beautiful. Conservatively, images in between are discarded.}
    \label{fig:Trueskill}
\end{figure}


\begin{figure*}[t!]
    \centering
    \hspace*{-5mm}
    \subfloat[]{
        \includegraphics[width=0.5\textwidth, height = 8cm ]{rotationalSim.png}
        \label{fig:rotSim}
    }
    \subfloat[]{
        \includegraphics[width=0.5\linewidth, height = 8cm ]{transSim.png}
        \label{fig:transSim}
    }
    \vspace{-0.4cm}
    \caption{Two types of augmentation: (a) rotation of the Street Views camera (based on rotation); and (b) exploration of scenes at increasing distances (based on translation).}
    \vspace{-0.4cm}
\end{figure*}



\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{SimilarityPlacesPrevalence.png}
    \caption{The types of scene that have greater propensity to be correctly augmented with similar scenes at increasing distances.}
    \label{fig:augmentationSimilarity}
\end{figure}




\begin{table}[t!]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Augmentation} & \textbf{Accuracy (Percentage)}\\
        \hline
        None & 63 \\
        \hline
        Rotation  & 68 \\
        \hline
        Rotation + Translation  & 64 \\
        \hline
        Rotation + Conservative Translation & 73.5 \\
        \hline
    \end{tabular}
    \caption{Percentage accuracy for our beauty classifier trained on differently augmented sets of  urban scenes.}
    \label{tab:classifier}
    \vspace{-10mm}
\end{table}


%****************************************
\subsection*{Step 2 Training a beauty classifier}
Having this highly curated set of labeled urban scenes, we are now ready to train classifier $C$ with labels reflecting our beauty assessments. We use the CaffeNet architecture, a modified version of AlexNet~\cite{krizhevsky2012imagenet,szegedy2015going}. The training is done on a 70\% split of the data, and the testing on the remaining 30\%. All this is done on increasingly augmented sets of data. We start from our 20k images and progressively augment them with  the snapshots obtained with the 5-angle camera rotations, and then with the exploration of scenes at increasing distance $d \in \{10,20,40,60\}$ meters. The idea behind data augmentation is that accuracy would increase with it. Indeed it does (Table~\ref{tab:classifier}): it goes from 63\% on the set of original scenes to as much as 73.5\% on the set of fully augmented scenes, which is a notable increase in accuracy for such classes of classification tasks. As a baseline, we compare with the models trained by Dubey et.al in \cite{dubey2016deep} on the same seed data that we use for our pipeline. They report that their models perform at 70\% accuracy in the task of picking a beautiful image amongst any two given images. Albeit the set-up of our model is not to compare two images but just to classify a particular image in a binary class, this baseline shows that our model is showing a comparable performance in beauty classification. 
%As a baseline, we compared with the scenic-or-not model developed by Chanuki et.al \cite{seresinhe2017using}. Their model is different than ours because of the fact that their model generates scenic scores on a scale of 10, whereas ours is a binary classifier. Still for the sake of comparison, we measure the Kendall rank correlation, which is what they use for their measurements, between predicted and actual classes. Our model performs with a Kendall rank correlation of 0.43 which is comparable to their elastic net models. In so far, the facelift pipeline is agnostic of the classifier , which can be swapped out with a better performing one or with one trained for a different use case. 
%\ns{Can you cite a baseline comparison from state of the art? Less important but comment on why is accuracy the right metric? Why not precision for instance?}. 


%\begin{figure*}[h]
%	\centering
%	\includegraphics[width=0.5\linewidth]{Plot/GanCompare.png}
%	\caption{Comparison of using the Default ImageNet GAN against Custom trained GAN for Activation maximization. By re-training the GAN on the test dataset, we can see improvement in terms of structure and colours in the generated images}
%	\label{fig:GanComparison}
%\end{figure*}


%****************************************
\subsection*{Step 3 Generating a synthetic beautified scene}
Having this trained classifier at hand, we can then build a generator of synthetic beautified scenes. This is a model that, given the two classes ugly $y_i$ and beautiful $y_j$, transforms any original scene $I_i$ of class $y_i$ (e.g., ugly scene) into template scene $\hat{I_j}$ that maximizes class $y_j$ (e.g., beautified template scene). 

More specifically, given an input image $I_i$ known to be of class $y_i$  (e.g., ugly), our technique outputs  $\hat{I_j}$, which is a more beautiful version of it (e.g., $I_i$ is morphed  towards the average representation of a beautiful scene) while preserving $I_i$'s details. The technique does so using the ``Deep Generator Network for Activation Maximization'' (\emph{DGN-AM}) \cite{nguyen2016synthesizing}. Given an input image $I_i$, \emph{DGN-AM} iteratively re-calculates the color of $I_i$'s pixels in  a way  the output image $\hat{I_j}$  both maximizes  the  activation of neuron $y_j$ (e.g., the ``beauty neuron'') and looks ``photo realistic'',  which is done by conditioning the maximization to an ``image prior''. This is equivalent to finding the feature vector $f$ that maximizes the following expression:
\begin{equation}
\hat{I_j} =G( f ) : \underset{f}{\arg\max}(C_{j}(G(f))-\lambda||f||)
\end{equation}
where:
\begin{itemize}
    \item $G(f)$ is the image synthetically generated from the candidate feature vector $f$;
    \item $C_j(G(f))$ is the activation value of neuron $y_j$ in the scene classifier $C$ (the value to be maximized);
    \item $\lambda$ is a $L_2$ regularization term.
\end{itemize}
Here the initialization of $f$ is key. If $f$ were to be initialized with random noise, then the resulting $G(f)$ would be the average representation of category $y_j$ (of, e.g., beauty). Instead, since $f$ is initialized with $I_i$, then the resulting $G(f)$ is $I_i$'s version ``morphed to become more beautiful''. 
%Figure \ref{fig:BeautyExample} shows the activation maximization output in the center.

%****************************************
\subsection*{Step 4 Returning a realistic beautified scene}
We now have template scene $\hat{I_j}$ (which is a synthetic beautified version of original scene $I_i$) and need to retrieve a realistic looking version of it. We do so by: \emph{i)} representing each of our original scenes in Step 1 (including $\hat{I_j}$) as a 4096 dimensional feature vector derived from the FC7 layer of the PlacesNet \cite{zhou2014learning}; \emph{ii)} computing the distance (as $L_2$ Norm) between $\hat{I_j}$'s feature vector and each of the original scene's feature vector; and \emph{iii)} selecting the original scene most similar (smaller distance) to $\hat{I_j}$. This results into the selection of the beautified scene $I_j$.


%****************************************
\subsection*{Step 5 Identifying  characterizing urban elements}
Since original scene $I_i$ and beautified scene $I_j$ are real scenes and we make sure that they maintain the same structural characteristics (e.g., point of view, layout), we can easily compare them in terms of presence or absence of SegNet's and PlacesNet's labels. That is, we can determine how the original scene and its beautified version differ in terms of urban design elements. This step required us to develop metrics inspired from urban design literature, to quantify the changes in elements. A detailed description of the characterization and evaluation would follow in Section \ref{sec:evaluation}


%\begin{figure*}[h]
%	\centering
%	\includegraphics[width=\linewidth]{Plot/Example.png}
%	\caption{Example of ``FaceLifting''.}
%	\label{fig:BeautyExample}
%\end{figure*}


\begin{table}
    \begin{tabular}{l*3{C}@{}}
        \toprule
        & Original ($I_i$) & Latent Beauty representation ($\hat{I_j}$) & Beautified ($I_j$) \\ 
        \midrule
        & \includegraphics[width=11em]{u_9} & \includegraphics[width=11em]{t_9} &  \includegraphics[width=11em]{b_9} \\ 
        & \includegraphics[width=11em]{u_4.jpeg} & \includegraphics[width=11em]{t_4.jpeg} &  \includegraphics[width=11em]{b_4.jpeg} \\ 
        & \includegraphics[width=11em]{u_5.jpeg} & \includegraphics[width=11em]{t_5.jpeg} &  \includegraphics[width=11em]{b_5.jpeg} \\ 
        & \includegraphics[width=11em]{u_7.jpeg} & \includegraphics[width=11em]{t_7.jpeg} &  \includegraphics[width=11em]{b_7.jpeg} \\ 
        & \includegraphics[width=11em]{u_8.jpeg} & \includegraphics[width=11em]{t_8.jpeg} &  \includegraphics[width=11em]{b_8.jpeg} \\ 
        \bottomrule 
    \end{tabular}
    \caption{ The table showcases examples of the``FaceLifting'' process. It is worth observing that the process of beautification prefers greenery, narrow roads and  pavements }
    \label{fig:BeautyExample}
\end{table} 



\section{Evaluation}
\label{sec:evaluation}

The goal of FaceLift is to transform existing urban scenes into versions that: \emph{i)} people perceive more beautiful; \emph{ii)} contain urban elements typical of great urban spaces; \emph{iii)} are easy to interpret; and \emph{iv)} architects and urban planners find useful. To ascertain whether FaceLift meets that composite goal, we answer the following questions next: 

\begin{description}
    \item{\textbf{Q1}} Do individuals perceive``FaceLifted'' scenes to be beautiful?
    
    \item{\textbf{Q2}}  Does our framework produce scenes that possess urban elements typical of great spaces?
    
    \item{\textbf{Q3}}  Which urban elements are mostly associated with beautiful scenes?
    
    \item{\textbf{Q4}}  Do architects and urban planners find FaceLift useful?
    
\end{description}


\subsection*{Q1 People's perceptions of beautified scenes}
To ascertain whether FaceLifted scenes are perceived by individuals as they are supposed to, we run a crowd-sourcing experiment on Amazon Mechanical Turk.  We randomly select 200 scenes, 100 beautiful and 100 ugly  (taken at the bottom 10 and top 10 percentiles of the Trueskill's score distribution of Figure~\ref{fig:Trueskill}). Our framework then transforms each ugly scene into its beautified version, and each beautiful scene into its corresponding `uglified'. These scenes are arranged into pairs, each of which contains the original scene and its beautified or uglified version. On  Mechanical Turk, we only select verified masters for our crowd-sourcing workers (those with an approval rate above 90\% during the past 30 days), pay them \$0.1 per  task,  and ask each of them to choose the beautiful scene for given pairs.  We make sure to have at least 3 votes for each scene pair. Overall, our workers end up selecting the scenes that are actually beautiful 77.5\% of the times, suggesting that FaceLifted scenes are correctly perceived most of the times.


\subsection*{Q2 Are beautified scenes great urban spaces?}
To answer that question, we need to understand what makes a space great. After a careful review of the urban planning literature, we identify four factors~\cite{ewing2013measuring,alexander1977pattern} (summarized in Table~\ref{tab:Design_metrics}): great places mainly tend to be walkable, offer greenery, feel cozy, and be visually rich. 


\begin{table*}[h]
    \centering
    
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{|c|p{10cm}|}
            \hline
            \textbf{Metric} & \textbf{Description}\\
            \hline
            Walkability  & Walkable streets increase the social capital of a place,  and they appeal to the exploring nature of the human psyche~\cite{ewing2013measuring,quercia15thedigital,speck12}.\\
            \hline
            Green Spaces & The presence of greenery has repeatedly been found to impact people's well being \cite{alexander1977pattern}. Under certain conditions, it could also promote social interactions~\cite{quercia2014aesthetic}. This suggest that not all greenery has to be considered in the same way though: dense forests or unkempt greens might well have a negative impact~\cite{jacobs1961death}. \\
            %			\hline		
            %			Landmarks & Loosing a bearing in the city is not a very pleasant experience. Hence presence of recognisable and  guiding landmarks influences the perception of an urban space \cite{ewing2013measuring}.\\
            \hline
            Privacy-Openness &  A sense of privacy (as opposed to a sense of openness) impacts a place's perception~\cite{ewing2013measuring}.\\ 
            \hline
            Visual Complexity & Visual complexity is a measure of how diverse a urban scene is in terms of design materials, textures, and objects~\cite{ewing2013measuring}.  \\
            \hline
    \end{tabular}}
    \caption{Urban Design Metrics}
    \label{tab:Design_metrics}
    %        \vspace{-5mm}
\end{table*}


To automatically extract visual cues related to these four factors, we select 500 ugly scenes and 500 beautiful ones at random, transform them into their opposite aesthetic qualities (i.e., ugly ones are beautified, and beautiful ones are `uglified'), and compare which urban elements related to the four factors distinguish uglified scenes from beautified ones. 

We extract labels from each of our 1,000 scenes using two image classifiers. First, using PlacesNet~\cite{zhou2014learning}, we label each of our scenes according to a classification containing 205 labels (reflecting, for example, landmarks, natural elements), and retain the five labels with highest confidence scores for the scene. Second, using Segnet~\cite{badrinarayanan2015segnet}, we  label each of our scenes according to a classification containing 12 labels. Segnet is trained on dash-cam images, and classifies each scene pixel with one of these twelve labels: road, sky, trees,  buildings, poles, signage, pedestrians, vehicles, bicycles, pavement, fences, and road markings. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{taxonomyCount.png}
    \caption{Number of labels in specific urban design categories (on the $x$-axis) found in beautified scenes as opposed to those found in uglified scenes.}
    \label{fig:taxonomyCount}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{walkable_taxonomy.png}
    \caption{Count of specific walkability-related labels  (on the $x$-axis) found in beautified scenes minus the count of the same labels found in uglified scenes.}
    \label{fig:WalkableTnomy}
\end{figure}


Having these two ways of labeling scenes, we can now test whether the expectations set by the literature describing metrics of great urban spaces (Table~\ref{tab:Design_metrics}) are  met in the FaceLifted scenes. 


%**************************************************
\mbox{ } \\
\noindent
\emph{H1 Beautified scenes tend to be walkable.}
We manually select only the PlacesNet labels that are related to walkability. These labels include, for example, \textit{abbey, plaza, courtyard, garden, picnic area, \textrm{and} park}. To test hypothesis \emph{H1}, we count the number of walkability-related labels found in beautified scenes as opposed to those found in uglified scenes (Figure~\ref{fig:taxonomyCount}): the former contain twice as many walkability labels than the latter. We then determine which types of scenes are associated with beauty (Figure~\ref{fig:WalkableTnomy}). Unsurprisingly, beautified scenes tend to show gardens, yards, and small paths. By contrast, uglified ones tend to show built environment features such as shop fronts and broad roads. 


\mbox{ } \\
%**************************************************
\noindent
\emph{H2 Beautified scenes tend to offer green spaces.}
We manually select only the PlacesNet labels that are related to greenery. These labels include, for example, \textit{fields, pasture, forest, ocean, and beach}. Then, in our 1,000 scenes, to test hypothesis \emph{H2}, we count the number of nature-related labels found in beautified scenes as opposed to those found in uglified scenes (Figure~\ref{fig:taxonomyCount}): the former contain more than twice as many nature-related labels than the latter.  To test this hypothesis further, we compute the fraction of `tree' pixels (using SegNet's label `tree') in beautified and uglified scenes, and  find that beautification adds  32\% of tree pixels, while uglification removes 17\% of them. 


\begin{figure*}[!t]
    \centering
    \hspace*{-5mm}
    \subfloat[]{
        \includegraphics[width=0.45\textwidth, height = 5cm ]{BinnedPlot.png}
        \label{fig:skyBinned}
    }
    \subfloat[]{
        \includegraphics[width=0.45\linewidth, height = 5cm ]{binnedPlot_complexity.png}
        \label{fig:complexity}
    }
    \vspace{-0.4cm}
    \label{fig:bin_figures}
    \caption{The percentage of scenes ($y$-axis): (a) having an increasing presence of sky (on the $x$-axis); and (b) having an increasing level of visual richness  (on the $x$-axis). The error bars represent standard errors obtained by random re-sampling of the data for 500 iterations. }
    \vspace{-0.4cm}
\end{figure*}



\mbox{ } \\
%**************************************************
\noindent
\emph{H3 Beautified scenes tend to feel private and `cozy'.}
To  test hypothesis \emph{H3}, we count the fraction of pixels that Segnet labeled  as `sky' and show the results in a bin plot in Figure~\ref{fig:skyBinned}:  the $x$-axis has six bins (each of which represents a given range of sky fraction), and the $y$-axis shows the percentage of beautified \emph{vs.} uglified scenes that fall into each bin.  Beautified scenes tend to be cozier (lower sky presence) than the corresponding original scenes.


\mbox{ } \\
%**************************************************
\noindent
\emph{H4 Beautified scenes tend to be visually rich.}
To quantify to which extent scenes are visually rich, we measure their visual complexity~\cite{ewing2013measuring} as  the amount of disorder in terms of distribution of (Segnet) urban elements in the scene: 
\begin{equation}
H(X) = -\sum p(i)\log p(i)
\label{eq:entropy} 
\end{equation}
where $i$ is the $i^{th}$ Segnet's label. The total number of labels is twelve. The higher $H(X)$, the  higher the scene's entropy, that is, the higher the scene's complexity. To test hypothesis \emph{H4}, we show the percentage of scenes that fall into a complexity bin  (Figure~\ref{fig:complexity}): beautified scenes are of low to medium complexity, while uglified ones are of high complexity.



%**************************************************
\subsection*{Q3 Urban elements of beautified scenes}

\begin{table}[t!]
    \centering
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \textbf{Pair of urban elements} & \textbf{$\beta_1$}  & \textbf{$\beta_2$} & \textbf{$\beta_3$}  & Error Rate (Percentage)\\
            \hline
            \hline
            Buildings - Trees & -0.032 & 0.084  & 0.005  & 12.7 \\
            \hline
            Sky - Buildings & -0.08 & -0.11 & 0.064 & 14.4 \\
            \hline
            Roads - Vehicles  & -0.015  & -0.05 & 0.023  & 40.6 \\
            \hline
            Sky - Trees & 0.03 & 0.11 & -0.012 & 12.8  \\
            \hline
            Roads - Trees & 0.04  & 0.10 &  -0.031  & 13.5  \\
            \hline
            Roads - Buildings & -0.05  & -0.097  &  0.04  & 20.2  \\
            \hline
        \end{tabular}
    }
    \caption{Coefficients of logistic regressions run on one pair of predictors at the time.}
    \label{tab:regressioncoef}
    \vspace{-10mm}
\end{table}


To determine which urban elements are the best predictors of urban beauty and the extent to which they are so, we run a logistic regression, and, to ease interpretation, we do so on one pair of predictors at the time: 
\begin{equation}
Pr(\textrm{beautiful}) = logit^{-1}(\alpha + \beta_1 * V_1 + \beta_2 * V_2  + \beta_3 * V_{1}.V_{2} )
\label{eq:regression} 
\end{equation}
where $V1$ is the fraction of the scene's pixels marked with one Segnet's label, say, ``buildings'' (over the total number of pixels),  and $V2$ is the fraction of the scene's pixels marked with another label, say, ``trees''. The result consists of three beta coefficients: $\beta_1$ reflects $V1$'s contribution in predicting beauty,  $\beta_2$ reflects $V2$'s contribution, and $\beta_3$ is the interaction effect, that is, it reflects the contribution of the dependency of $V1$ and $V2$ in predicting beauty. We run logistic regressions on the five factors that have been found to be most predictive of urban beauty~\cite{quercia2014aesthetic, ewing2013measuring, alexander1977pattern}, and show the results in Table~\ref{tab:regressioncoef}.


Since we are using logistic regressions, the quantitative interpretation of the beta coefficients is eased by the ``divide by 4 rule''~\cite{vaughn2008data}: we can take $\beta$ coefficients and ``divide them by 4 to get an upper bound of the predictive difference corresponding to a unit difference'' in beauty~\cite{vaughn2008data}. For example, take the results in the first row of Table~\ref{tab:regressioncoef}. In the model $Pr(beautiful) = logit^{-1}(\alpha - 0.032 \cdot buildings + 0.084 \cdot trees + 0.005 \cdot  buildings \cdot trees)$, we can divide - 0.032/4 to get -0.008: a difference of 1 in the fraction of pixels being buildings corresponds to no more than a 0.8\% \emph{negative} difference in the probability of the scene being beautiful. In a similar way, a difference of 1 in the fraction of pixels being trees corresponds to no more than a 0.021\% \emph{positive} difference in the probability of the scene being beautiful. By considering the remaining results in Table~\ref{tab:regressioncoef}, we find that, across all pairwise comparisons, trees is the most positive element associated with beauty, while roads and buildings are the most negative ones. Since these results go in the direction one would expect, one might conclude that the scenes beautified by our framework are in line with previous literature, adding further external validity to our work. 




%**************************************************
\subsection*{Q4 Do architects and urban planners find it useful?}

\begin{table}[t!]
    \centering
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            \textbf{Use case} & \textbf{Definitely Not}  & \textbf{Probably Not} & \textbf{Probably}  & \textbf{Very Probably} & \textbf{Definitely}\\
            \hline
            \hline
            Decision Making & 4.8\% & 9.5\%  & 38\%  &  28.6\% & 19\%\\
            \hline
            Participatory Urban Planning & 0\% & 4.8\%  & 52.4\%  &  23.8\% & 19\%\\
            \hline
            Promote Green Cities & 4.8\% & 0\%  & 47.6\%  &  19\% & 28.6\%\\
            \hline
        \end{tabular}
    }
    \caption{Urban experts polled about the extent to which an interactive map of ``FaceLifted'' scenes promotes: (a) decision making; (b) citizen participation in urban planning; and (c) promotion of green cities}
    \label{tab:useCases}
    \vspace{-10mm}
\end{table}


%\begin{figure*}[!t]
%	\centering
%	\hspace*{-5mm}
%	\subfloat[]{
%		\includegraphics[width=0.3\textwidth ]{Plot/DecisionMaking.png}
%		\label{fig:decision}
%	}
%	\subfloat[]{
%		\includegraphics[width=0.3\linewidth ]{Plot/ParticipationUrbanPlanning.png}
%		\label{fig:participation}
%	}
%	\subfloat[]{
%		\includegraphics[width=0.3\linewidth ]{Plot/PromoteGreenCities.png}
%		\label{fig:promotion}
%	}
%%	\vspace{-0.4cm}
%	\caption{Urban experts polled about the extent to which an interactive map of ``FaceLifted'' scenes promotes: (a) decision making; (b) citizen participation in urban planning; and (c) promotion of green cities.\ns{Pie charts are not advisible. The labels are too small. You can perhaps replace this with a table.}}
%	\label{fig:pies}
%	\vspace{-0.4cm}
%\end{figure*}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{UI.png}
    \caption{Interactive map of FaceLifted scenes in Boston.}
    \label{facelift-UI}
\end{figure}



To ascertain whether practitioners find FaceLift potentially useful, we built an interactive map of the city of Boston in which, for  selected points, we showed pairs of urban scenes before/after beautification (Figure~\ref{facelift-UI}). We then sent that map along with a survey to 20 experts in architecture, urban planning, and data visualization around the world.  The experts had to complete tasks in which they rated FaceLift based on how well it supports decision making, participatory urbanism, and promotion of green spaces among the general public. The results are show in Table~\ref{tab:useCases} according to our experts, the tool can very probably supports decision making, probably support participatory urbanism, and definitely promote green spaces.  These results are  qualitatively supported by our experts' comments, which include: ``\textit{The maps reveal patterns that might not otherwise be apparent}'',  ``\textit{The tool helps focusing on parameters to identify beauty in the city while exploring it}'',  and ``\textit{The metrics are nice. It made me think more about beautiful places needing a combination of criteria, rather than a high score on one or two dimensions. It made me realize that these criteria are probably spatially correlated}''.









\section{Conclusion}
\label{sec:discussion}

FaceLift is a transparent framework that beautifies existing urban scenes. This translates into two main technical advancements. First, FaceLift is able to generate realistic scenes as opposed to existing approaches based on  Generative Adversarial Networks whose final transformations are quite coarse as they still take the form of synthetic templates.  Second, it augments the deep learning black-box with a module that offers explanations on what has been transformed, making that box more transparent. 

There are still important limitations though. One is data bias. The framework is as good as its training data, and more work has to go into collecting reliable ground truth of human perceptions. This data should ideally be stratified according to the people's characteristics that  impact their perceptions. The other main limitation is that generative models are hard to control, and more work has to go into offering principled ways of fine-tuning the generative process.

Despite these limitations, FaceLift has the potential to support urban interventions  in scalable  and replicable ways: it can be applied to the scale of an entire city, and that  can be replicated in other cities. The advantage of shifting the focus of research away from predictive analytics towards urban interventions is that people could be part of discussions on works of architecture  more than they are nowadays. To turn existing spaces into something more beautiful, that will still be the duty of architecture. Yet, with technologies similar to FaceLift more readily integrated in the architecture discussions, the complex job of recreating restorative spaces in an increasingly urbanized world will be greatly simplified.  After all, ``we delight in complexity to which genius have lent an appearance of simplicity.''~\cite{de2008architecture} In the context of future work, that genius is represented by the future technologies that we will contribute to build to deal with the complexity of our cities.



